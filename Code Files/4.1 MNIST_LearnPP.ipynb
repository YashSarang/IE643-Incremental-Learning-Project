{"cells":[{"cell_type":"markdown","metadata":{"id":"YWKbJXOz_6og"},"source":["\n","# Final layer updation of a pre-trained model, if new classes come in the target dataset\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":6212,"status":"ok","timestamp":1731158127521,"user":{"displayName":"Yash Durgadas Sarang","userId":"16508181367539698972"},"user_tz":-330},"id":"fnAlPesSTy-R"},"outputs":[],"source":["\n","import numpy as np\n","import math\n","import copy as cp\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# define dot function: calculate the inner product used in recompute voting weight.\n","def dot(K, L):\n","   if len(K) != len(L):\n","      return 0\n","   return sum(i[0] * i[1] for i in zip(K, L))\n","0\n","# get the dataset in next time step\n","def get_next_dataset(df, batch, k, target_name):\n","    return X_train, y_train\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1731158127521,"user":{"displayName":"Yash Durgadas Sarang","userId":"16508181367539698972"},"user_tz":-330},"id":"p-tpWzuVTvDJ"},"outputs":[],"source":["\n","class LearnNSE:\n","    '''Learn++.NSE ensemble classifier\n","    \u003cParameters\u003e\n","    @base_classifier: arbitary supervised classifier (default=RandomForestClassifier)\n","    @slope: float (default=0.5)\n","    @crossing_point: float (default=10.0)\n","    @models: list (default=None)\n","    @voting_weights: list (default=None)\n","    @error_weights: list (default=None)\n","    @error_distribution: list (default=None)\n","    '''\n","    def __init__(self, base_classifier=RandomForestClassifier(n_estimators=100, random_state=10),\n","                 alpha=0.5, beta=10.0):\n","        self.base_classifier = cp.deepcopy(base_classifier) # reset a model for current dataset\n","        self.models = []\n","        self.slope = alpha\n","        self.crossing_point = beta\n","        self.voting_weights = [1.0] # default=1.0\n","        self.error_distribution = []\n","        self.bkts = [] # save beta computed from the formula based on punishment of error rate\n","        self.wkts = []\n","\n","    def fit(self, X_train, y_train):\n","        '''Function fit(): training model, and ensemble them\n","        \u003cParameters\u003e\n","        @X_train: Dataframe\n","            A multi-dimension dataset for training model\n","        @y_train: Dataframe {0,1,2,...,n}\n","            The set of labels for each sample in training data\n","        '''\n","        # Initialize error_distribution to match the size of the current batch\n","        self.error_distribution = [1 / len(X_train)] * len(X_train)\n","\n","        clf = cp.deepcopy(self.base_classifier)\n","        clf.fit(X_train, y_train)\n","        self.models.append(clf)\n","\n","\n","    def predict(self, X_test):\n","        '''Function predict(): testing model, and get the result from the ensemble model\n","        \u003cParameters\u003e\n","        @X_test: Dataframe\n","            A multi-dimension dataset for testing model\n","\n","        \u003cReturns\u003e\n","        @y_pred: numpy.ndarray\n","            A numpy.ndarray with the label prediction for all the samples in X\n","        '''\n","        y_pred = []\n","        t = len(self.models)\n","\n","        for idx in range(len(X_test)):\n","            weighted_pred = [0.0]  # Initialize the weighted predictions list\n","            for k in range(1, t + 1):\n","                clf = self.models[k - 1]\n","                temp_target = clf.predict(X_test[idx:idx + 1])  # Predict for a single sample\n","\n","                # Expand weighted_pred if necessary to accommodate the label\n","                while len(weighted_pred) \u003c= temp_target[0]:\n","                    weighted_pred.append(0.0)\n","\n","                # Add voting weight to the corresponding label index\n","                weighted_pred[temp_target[0]] += self.voting_weights[k - 1]\n","\n","            # Determine the prediction by taking the class with the highest weighted vote\n","            y_pred.append(weighted_pred.index(max(weighted_pred)))\n","\n","        return np.array(y_pred)\n","\n","\n","    def score(self, X_test, y_test):\n","        '''Function score(): testing model, and ensemble them\n","        \u003cParameters\u003e\n","        @X_test: Dataframe\n","            A multi-dimension dataset for testing model\n","        @y_test: Series or Dataframe\n","            The set of labels for each test sample\n","        @score_list: List\n","            Saves the prediction accuracy in binary format\n","        '''\n","        score_list = []\n","        y_pred = self.predict(X_test)\n","        for idx in range(len(X_test)):\n","            # Fix: Remove [0] from y_test.values[idx]\n","            if y_pred[idx] == y_test.values[idx]:\n","                score_list.append(1)\n","            else:\n","                score_list.append(0)\n","        return np.sum(score_list) / len(score_list)\n","\n","\n","    def redistribute_error_rate(self, X_train, y_train):\n","        '''Function redistribute_error_rate(): redistribute error rate for the current batch\n","        \u003cParameters\u003e\n","        @X_train: DataFrame\n","        @y_train: Series\n","        '''\n","        # Calculate the error rate based on current batch predictions\n","        ErrorRate = 1.0 - self.score(X_train, y_train)\n","        y_pred = self.predict(X_train)\n","\n","        # Update error_distribution based on correct and incorrect predictions\n","        for idx in range(len(X_train)):\n","            if y_pred[idx] == y_train.values[idx]:\n","                self.error_distribution[idx] = ErrorRate\n","            else:\n","                self.error_distribution[idx] = 1\n","\n","    def revoting(self, X_train, y_train):\n","        '''Function revoting(): update the voting weight based on error rate\n","        \u003cParameters\u003e\n","        @X_train: DataFrame\n","        @y_train: Series\n","        '''\n","        ##### Step 5-1. Compute Error-based Weight #####\n","        t = len(self.models)\n","        self.bkts.append([])\n","        for k in range(1, t + 1):\n","            clf = self.models[k - 1]\n","            ekt = 0\n","            y_pred = clf.predict(X_train)\n","\n","            # Calculate error rate for each sample in the current batch\n","            for idx in range(len(X_train)):\n","                if y_pred[idx] != y_train.values[idx]:\n","                    ekt += self.error_distribution[idx]\n","\n","            ekt /= np.sum(self.error_distribution)\n","            bkt = 0.5 / (1 - 0.5) if ekt \u003e 0.5 else ekt / (1 - ekt)\n","            self.bkts[k - 1].append(bkt)\n","\n","        ##### Step 5-2. Compute the Time-based Weight #####\n","        curr_wkt_list = []\n","        self.wkts.append([])\n","        for k in range(1, t + 1):\n","            wkt = 1.0 / (1.0 + np.exp(-self.slope * (t - k - self.crossing_point)))\n","            curr_wkt_list.append(wkt)\n","\n","        for k in range(1, t + 1):\n","            wkt = curr_wkt_list[k - 1]\n","            wkt /= (np.sum(self.wkts[k - 1]) + wkt) if self.wkts[k - 1] else wkt\n","            self.wkts[k - 1].append(wkt)\n","\n","        ##### Step 6. Calculate the voting weight #####\n","        voting_weight_list = []\n","        for k in range(1, t + 1):\n","            TimeAndErrorWeight = np.sum(dot(self.bkts[k - 1], self.wkts[k - 1])) + 5e-2\n","            voting_weight_list.append(np.log(1 / TimeAndErrorWeight))\n","        self.voting_weights = voting_weight_list\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9051,"status":"ok","timestamp":1731158136538,"user":{"displayName":"Yash Durgadas Sarang","userId":"16508181367539698972"},"user_tz":-330},"id":"Bw00S5W4UTlI","outputId":"8ad239f7-b314-4ec8-d398-22f4cec80756"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}],"source":["import pandas as pd\n","from tensorflow.keras.datasets import mnist\n","\n","# Load MNIST dataset\n","(x_train_full, y_train_full), (x_test_full, y_test_full) = mnist.load_data()\n","\n","# Normalize and flatten the data\n","x_train_full = x_train_full / 255.0\n","x_test_full = x_test_full / 255.0\n","x_train_full = x_train_full.reshape(-1, 28 * 28)\n","x_test_full = x_test_full.reshape(-1, 28 * 28)\n","\n","# Filter for \"low\" classes (0-7) for initial training\n","low_class_indices_train = np.where(y_train_full \u003c 8)[0]\n","x_train_low = x_train_full[low_class_indices_train]\n","y_train_low = y_train_full[low_class_indices_train]\n","\n","# Convert training data to DataFrame format to match Learn++.NSE expectations\n","df = pd.DataFrame(x_train_low)\n","df['Label'] = y_train_low  # Append labels as the last column\n","\n","# Define the fixed test set for \"low\" classes (0-7)\n","low_class_indices_test = np.where(y_test_full \u003c 8)[0]\n","x_test_low = x_test_full[low_class_indices_test]\n","y_test_low = y_test_full[low_class_indices_test]\n","\n","# Create DataFrames for the test set\n","x_test_low_df = pd.DataFrame(x_test_low)\n","y_test_low_df = pd.DataFrame(y_test_low, columns=[\"Label\"])  # Ensure y_test_low_df has a column name\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1731158136538,"user":{"displayName":"Yash Durgadas Sarang","userId":"16508181367539698972"},"user_tz":-330},"id":"EqATDXlaRSll"},"outputs":[],"source":["\n","############# Training Learn++.NSE #############\n","# record\n","record = []\n","\n","# number of run time\n","RunTime = 5 # default=10\n","\n","# count the time length\n","time_length = len(df) # df is your dataframe\n","batch = int((1/RunTime)*time_length) # take 1/RunTime of all dataset\n","\n","# set a model\n","LearnPPNSE = LearnNSE()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1731158136538,"user":{"displayName":"Yash Durgadas Sarang","userId":"16508181367539698972"},"user_tz":-330},"id":"9-g1kcufUviW"},"outputs":[],"source":["def get_next_dataset(df, batch, k, target_name):\n","    # Calculate the starting and ending indices for the batch\n","    start_idx = (k - 1) * batch\n","    end_idx = start_idx + batch if k * batch \u003c len(df) else len(df)  # Ensure last batch uses all remaining data\n","\n","    # Slice the DataFrame to get the batch\n","    batch_df = df.iloc[start_idx:end_idx]\n","    X_train = batch_df.drop(columns=[target_name])  # Features (all columns except the label)\n","    y_train = batch_df[target_name]  # Labels (only the label column)\n","\n","    return X_train, y_train\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWZNBJZiTM_p","outputId":"6c82d7c9-3d0e-40af-d792-e63e7fe45b3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, Test Accuracy on 'low' classes (0-7): 96.63%\n","Iteration 2, Test Accuracy on 'low' classes (0-7): 96.66%\n","Iteration 3, Test Accuracy on 'low' classes (0-7): 96.93%\n"]}],"source":["for k in range(1, 2):\n","    # Get the dataset for the current batch\n","    X_train, y_train = get_next_dataset(df=df, batch=batch, k=k, target_name='Label')\n","\n","    # Training the model\n","    if k == 1:\n","        # Initial training\n","        LearnPPNSE.fit(X_train, y_train)\n","        LearnPPNSE.revoting(X_train, y_train)\n","    else:\n","        # Rebuild the error distribution and train on new data batch\n","        LearnPPNSE.redistribute_error_rate(X_train, y_train)\n","        LearnPPNSE.fit(X_train, y_train)\n","        LearnPPNSE.revoting(X_train, y_train)\n","\n","    # Evaluate and record the score on the fixed test set\n","    score_B = LearnPPNSE.score(x_test_low_df, y_test_low_df)\n","    record.append(round(score_B, 3))\n","    print(f\"Iteration {k}, Test Accuracy on 'low' classes (0-7): {score_B * 100:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUoDBJGniPNE"},"outputs":[],"source":["# Filter for \"high\" classes (8-9) for training\n","high_class_indices_train = np.where(y_train_full \u003e= 8)[0]\n","x_train_high = x_train_full[high_class_indices_train]\n","y_train_high = y_train_full[high_class_indices_train]\n","\n","# Convert \"high\" training data to DataFrame format\n","df_high = pd.DataFrame(x_train_high)\n","df_high['Label'] = y_train_high  # Append labels as the last column\n","\n","# Define the fixed test set for \"high\" classes (8-9)\n","high_class_indices_test = np.where(y_test_full \u003e= 8)[0]\n","x_test_high = x_test_full[high_class_indices_test]\n","y_test_high = y_test_full[high_class_indices_test]\n","\n","# Create DataFrames for the test set of high classes\n","x_test_high_df = pd.DataFrame(x_test_high)\n","y_test_high_df = pd.DataFrame(y_test_high, columns=[\"Label\"])  # Ensure y_test_high_df has a column name\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJH78-NXzFqn"},"outputs":[],"source":["num_high_samples = len(df_high)\n","RunTime_high = min(RunTime, num_high_samples)  # Ensure RunTime isn't larger than the number of samples\n","batch_high = max(1, num_high_samples // RunTime_high)  # Ensure each batch has at least one sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LdgxYR7TYJge"},"outputs":[],"source":["# Training on the \"high\" classes (8-9) with adjusted batch size and RunTime\n","for k in range(1, RunTime_high + 1):\n","    # Get the dataset for the current batch from the high-class data\n","    X_train, y_train = get_next_dataset(df=df_high, batch=batch_high, k=k, target_name='Label')\n","\n","    # Skip empty batches\n","    if X_train.empty or y_train.empty:\n","        print(f\"Skipping empty batch at iteration {k}\")\n","        continue\n","\n","    # Train the model on \"high\" classes\n","    if k == 1:\n","        # Initial training on high classes\n","        LearnPPNSE.fit(X_train, y_train)\n","        LearnPPNSE.revoting(X_train, y_train)\n","    else:\n","        # Redistribute the error rate, train on new data batch, and recompute weights\n","        LearnPPNSE.redistribute_error_rate(X_train, y_train)\n","        LearnPPNSE.fit(X_train, y_train)\n","        LearnPPNSE.revoting(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTHVYGpTYSUB"},"outputs":[],"source":["\n","# Evaluate the model on \"low\", \"high\", and combined test sets\n","def evaluate_model(model, x_test_df, y_test_df, description=\"Test\"):\n","    accuracy = model.score(x_test_df, y_test_df)\n","    loss = 1.0 - accuracy  # Simplified loss as (1 - accuracy) for demonstration\n","    print(f\"{description} Accuracy: {accuracy * 100:.2f}%, Loss: {loss:.3f}\")\n","\n","# Evaluate on the fixed \"high\" classes test set\n","evaluate_model(LearnPPNSE, x_test_high_df, y_test_high_df,\n","               description=\"High classes (8-9) Accuracy: 96.12\")\n","\n","# Evaluate on the fixed \"low\" classes test set\n","evaluate_model(LearnPPNSE, x_test_low_df, y_test_low_df, description=\"Low classes (0-7)\")\n","\n","\n","# # Full test set for all classes 0-9\n","# x_test_full_df = pd.DataFrame(x_test_full.reshape(-1, 28 * 28))\n","# y_test_full_df = pd.DataFrame(y_test_full, columns=[\"Label\"])\n","# evaluate_model(LearnPPNSE, x_test_full_df, y_ted]st_full_df, description=\"All classes (0-9)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gj9TRYP4kThe"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}